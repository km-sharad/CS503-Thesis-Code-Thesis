\documentclass [11pt,letterpaper ,twoside ,openany ]{report}

\title{Localizing Little Landmarks with Transfer Learning}
\author{Sharad Kumar}

\usepackage{setspace}
\usepackage{geometry}
\geometry{margin=1in}

\usepackage{sectsty}
\chapternumberfont{\Large} 
\chaptertitlefont{\huge}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[symbol]{footmisc}
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{commath}

\usepackage{float}
%\floatstyle{boxed} 
%\restylefloat{figure}
\usepackage{chngcntr}
\counterwithout{figure}{chapter}
\counterwithout{table}{chapter}

\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{amsfonts}

%\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{wrapfig}

\begin{document}
  \maketitle
  \tableofcontents

    \begin{abstract}
    Locating a small object in an image --- like a mouse on a computer desk or the door handle of a car --- is an important computer vision problem to solve because in many real life situations a small object may be the first thing that gets operated upon in the image scene. While a significant amount of artificial intelligence and machine learning research has focused on localizing prominent objects in an image, the area of small object detection has remained less explored. In my research I explore the possibility of using context information to localize small objects in an image. Using a Convolutional Neural Network (CNN), I create a regression model to detect a small object in an image where model training is supervised by coordinates of the small object in the image. Since small objects do not have strong visual characteristics in an image, it's difficult for a neural network to discern their pattern because their feature map exhibits low resolution rendering a much weaker signal for the network to recognize. Use of context for object detection and localization has been studied for a long time. This idea is explored by Singh \textit{et al}.\ [1] for small object localization by using a multi-step regression process where  spatial context is used effectively to localize small objects in several datasets. I extend the idea in this research and demonstrate that the technique of localizing in steps using contextual information when used with transfer learning can significantly reduce model training time.
    \end{abstract}    

    \listoffigures
    \listoftables

    \chapter{Introduction}
    \doublespacing
    We use myriad small objects everyday without noticing their presence. We manipulate an inconspicuous car door handle when interacting with the car, use a barely visible light switch as we enter a dark room and a computer mouse may be the first object we operate when we sit down to use a computer. Despite the usefulness of small objects, their detection in an image is a relatively unexplored area in computer vision. Detection of small objects in an image is a difficult task, since these objects often do not have distinctive appearance. While Convolutional Neural Network (CNN) architectures have demonstrated superior performance for classification or localization of salient objects in an image, these architectures have not been very effective for identifying objects or relationships that occupy a small part of an image, since low resolution of small objects provide weak signal to the network for recognition. Another difficulty in small object detection, as noted by Chen \textit{et al}.\ [6], is that the precision requirement for accurate localization of small objects is several magnitudes higher as compared to large objects. The bounding box of a small object is much smaller as compared to the bounding box of a large object in an image. Therefore, the available pixels that the predicted bounding box of a small object in an image can deviate from the actual bounding box is much smaller when compared to the bounding box of a large object in an image.

    Singh \textit{et al}.\ [1] propose a recurrent CNN architecture that uses successively more relevant contextual information in a sequence of steps to localize small objects. Singh \textit{et al}.\ call these small objects \textit{little landmarks}. The training in this regression model is supervised by coordinates of the little landmark. The final step in this scheme predicts the small object's location, while prior steps predict where to look next for localization. The learning therefore discovers globally distinctive patterns to start the sequence and conditionally distinctive ones, called \textit {latent landmarks}, to get closer to the target in discrete steps.

    In this thesis I recreate the model proposed by Singh \textit{et al}.\ using TensorFlow\textsuperscript{\textregistered} [16]. I train the model on the same dataset that was used by Singh \textit{et al}.\ The parameters in the original model are randomly initialized and the model is trained end-to-end. I propose a modification to the model by incorporating \textit {transfer learning}. Transfer learning is a process where image features learned by a trained network can be used by another network for its own training. This techniques may result in reducing training time and may also be useful when there's is dearth of training data, as I explain in section 5.1 of this thesis. My goals in this thesis are following: i) test transfer learning as a way to speed up training of this system and observe the effect of fewer training examples on test accuracy of the model; and ii) to test generality of their model by training it on a new dataset whose spatial characteristics are little different from the datasets used by Singh \textit{et al}.\

    Singh \textit {et al}.\ demonstrate the robustness of their model by localizing small objects in several datasets, such as an electrical switch on a wall, a car door handle and the beak of a bird. Their results and the design philosophy that exploits context information for small object detection provides strong evidence that this technique can also be used for localization of other small objects in an image, since context is an important component that often helps in localizing an object in an image. However, as Singh \textit{et al}.\ train their model from scratch, their model takes a long time to train and needs a large amount of data for training. These limitations of their model was my motivation to modify their design and use transfer learning to build a more efficient model. Using transfer learning, generalizations learned by a model that has been trained on a large dataset can be effectively used as input activations to some other model. To exploit transfer learning, I modify the original model by repurposing a pre-trained VGG-16 [2] model where I use activations from \textit{pool4} layer of the VGG-16 model to initialize part of the model and with experiments I demonstrate that incorporating  transfer learning to the model in such a way significantly reduces training time without affecting the model accuracy. VGG-16 is a deep CNN model created by K. Simonyan and A. Zisserman from University of Oxford that achieved 92.7\% accuracy in top-5 test categories and 70.5\% in top-1 test category on the Imagenet dataset [3]. Imagenet is a dataset of over 14 million images belonging to 1000 categories. I demonstrate significant improvement in training time with my approach and report results in section 5.2.

    Researchers at Portland State University have created a labeled dataset of dog walking images. Chapter 5 provides statistics of this dataset. I use this dataset to test the effectiveness of little landmark CNN model to localize ``hand-holding-leash'' relationship in a dog walking image. Successful localization of this little landmark would be useful for the overall task of detecting relevant parts of ``walking a dog'' visual situation (Figure 1), such as likely location of the dog and the person walking the dog in the image.

    \begin{wrapfigure}{l}[9mm]{0.5\textwidth}
        \begin{center}
            \includegraphics[width=0.48\textwidth]{Images/Figure9-DWI}
        \end{center}
        \caption{Dog Walking Image from PSU Dog Walking dataset. Detection of ``hand-holding-leash'' little landmark can help localize the dog walker and the dog in the image. \textit{This figure is best viewed in color.}}
    \end{wrapfigure}    

    I performed four main experiments on the little landmark CNN model. In the first experiment, I recreated the model using TensorFlow\textsuperscript{\textregistered} [16] and trained it from scratch using the car door handle dataset to localize a car door handle in the image of a car. The purpose of this experiment was to demonstrate that my model faithfully replicates the original model. The test accuracy for this model was similar to that reported by Singh \textit{et al}.\ In the second experiment, I used transfer learning to modify the model and observed significant improvement in training time for the same dataset and achieved the same accuracy as in the first experiment. In the third experiment I trained the same model as in the second experiment, but localized on ``hand-holding-leash'' landmark in dog walking images. The localization in the third experiment performed poorly. The size of data available for in experiment 3 was about 20\% of the size of data used in experiment 2. In the last experiment, I repeated experiment 2, but with same size of data that was used in experiment 3. The test accuracy in this case was inferior than that of of experiment 2, but better than experiment 3. I discuss these experiments and results in more detail in section 5.2.

    The remainder of this thesis is organized as follows. I discuss related work in chapter 2. Chapter 3 contains a brief discussion of CNN, the mechanics of training a CNN model and how regularization is used in finding a balance between bias and variance to minimize generalization error. In Chapter 4 I explain CNN architecture for the little landmark localization and also discuss the prediction model for little and latent landmarks. In chapter 5 I discuss my model modifications and present the results of my experiments. I conclude in Chapter 6 where I summarize my research findings and Chapter 7 has a brief discussion about possible extensions to this work.

    \chapter{Background}
    \doublespacing
    Since its resurgence in 2012, when a neural network based architecture called AlexNet was proposed by Krizhevsky \textit{et al}.\ [26] for image classification of ImageNet [3] dataset, a variant of neural network architecture called Convolutional Neural Network (CNN) has achieved significant improvement in image classification as well as regression tasks. Another influential work in this area is done by Girshick \textit{et al}.\ [28] where they propose an architecture that uses region proposals with CNN for object localization. Their method is called R-CNN. Understanding of CNN architecture and object detection scheme in R-CNN architecture is necessary for understanding little landmark localization system proposed by Singh \textit{et al}.\ and my extension to their architecture. In this chapter I provide an overview of CNN and R-CNN architectures. 

     \section{Convolutional Neural Network (CNN)}
     CNN is a deep learning \footnote {Deep Learning AI systems can be represented as a deep graph with many layers that represent hierarchical structure of the learning network. These systems understand the world in terms of a hierarchy of concepts, with each concept defined through its relation to simpler concepts [13].} model and is essentially a special kind of neural network that can be used to solve classification or regression problems. In a tradition neural network, parameter values of the model are stored in a two-dimensional matrix and the interaction between input and output layers of the models is encoded as the result of multiplication operation between the parameter matrix and a one-dimensional input vector, whereas the connectivity between input and output units in a CNN is partial in the sense that an input unit is connected only to a few output units --- via a weight vector (called kernel or filter). Sparse interactions between input and output units in a CNN results in an architecture where training is much more efficient because of reduced number of parameters in the model as compared to the number of parameters in the traditional neural network of same depth. Another important design characteristic of a CNN is that weights in a kernel are shared across all interactions, which further reduces the number of parameters in the model. I explain CNN architecture in more detail next. 

    \begin{figure}[h]
      \centering
      \begin{subfigure}[b]{1.00\linewidth}
        \includegraphics[width=\linewidth]{Images/Figure1-CNN}
      \end{subfigure}
      \caption{Schematic diagram of a Convolutional Neural Net (CNN). A typical layer of a CNN has three components - convolution (CONV), non-linearity (ReLU) and pooling (POOL). Many such layers are stacked together to create a deep architecture. Input enters the architecture from the left (image of the car) and after going thru many layers of CONV-ReLU-POOL transformations produces output (image classification) in the rightmost layer. In front part of the architecture (feature learning), inputs and outputs are partially connected, whereas towards the end learned features of the model are passed thru a fully connected layer to generate a probability distribution for classification classes or a real value for a regression problem. \textit{This figure is best viewed in color.}}
      \label{fig:cnn}
    \end{figure}

    \begin{wrapfigure}{l}{0.5\textwidth}
        \begin{center}
            \includegraphics[width=0.48\textwidth]{Images/Figure2-CNNFilter}
        \end{center}
        \caption{Dot product of the \textit{3 X 3} kernel with an input of dimension \textit{5 X 5} produces an output of \textit{3 X 3} dimension. Dot product of the kernel with nine cells in top left corner of the input produces the output in top left cell of the output. \textit{This figure is best viewed in color.}}
    \end{wrapfigure}    

    Figure 2 \footnote{Image source: https://www.mathworks.com/solutions/deep-learning/convolutional-neural-network.html} shows a schematic diagram of a general CNN architecture. A layer of CNN typically has convolution (CONV), non-linearity (Rectified Linear Unit --- ReLU) and pooling (POOL) stages. CNN layers are arranged in a sequence and in this hierarchical model, each layer learns features or representations at increasing levels of abstraction. The input enters the model from left and in this example is classified as one of several image classes by the rightmost layer of the model. However, this architecture can also be used to solve a regression problem where the output is a single continuous value. As depicted in Figure 2, a small patch of input generates a small patch of output in the CONV-ReLU stage. The generated output is called a feature map or activations. These activations are generated using a kernel as shown in Figure 3. Figure 3 shows a \textit{5 X 5} input when processed with a  \textit{3 X 3} filter produces an output of dimension \textit{3 X 3}. The output is generated by taking the dot product of the kernel with the input as it slides (convolves) over the input, in increments of one cell in this example. In Figure 3 the dot product of nine cells in top left corner of the input with the kernel produces top left cell of the output. Rest of the values in the output are produced as the the kernel convolves horizontally and vertically one cell at a time. This one cell movement of the kernel is called a \textit{stride}. The value of the stride can be anything between one and the length of the kernel. Sometimes the input is zero padded so that the kernel aligns with the input as is convolves horizontally and vertically across the input.

    The weight values in a kernel do not change as it convolves across the input. The height and length of feature map produced by the  CONV stage of a layer is determined by the the kernel size and the stride. However, the depth of the feature map is determined by the architecture designer. The depth of the convolution dictates how many filters there are in what's called a filter bank. The rational for this design is that different filters in the filter bank of the convolution can learn different characteristics of the input. The fact that different filters in a filter bank learn different features of the input image and also that higher layers of the model learn features by using features learned by lower layers was demonstrated by Fergus \textit{et al}.\ [4, 5] using a \textit{Deconvolutional Network} that maps activations back to the input pixel space. 

    The CONV stage in a CNN is followed by the ReLU stage that introduces non-linearity into the model by applying \(max (0,x)\) element-wise to the activations produced by kernels in the convolution layer. Introducing non-linearity with ReLU ensures that the CNN model does not collapse into a large linear model [27, 12, 29]. A CNN model (and neural networks in general) can therefore be thought of as a nonlinear generalization of a linear model, which by introducing the nonlinear transformation greatly enlarges the class of the linear model [29]. This feature of CNN allows it to fit the model using a non-linear function that is more suitable (in terms of lower test error) for the domain represented by the training dataset. Use of \(max (0,x)\) non-linear function results in accelerated training as compared to logistic or hyperbolic tangent function as explained by Krizhevsky \textit{et al}.\ [26]. The gradient in the saturating part of logistic or hyperbolic tangent activation functions becomes very small (for very large or very small weights) resulting in sluggish training because of minuscule weight change during training.

    \begin{wrapfigure}{r}{0.5\textwidth}
        \begin{center}
            \includegraphics[width=0.48\textwidth]{Images/Figure3-Pooling}
        \end{center}
        \caption{Pooling is used to create summary of activations generated by the ReLU layer. This figure shows two types of pooling commonly used in CNN. In average pooling, the average value of some output activations are used as a representative and the maximum value is used in max-pooling. \textit{This figure is best viewed in color.}}
    \end{wrapfigure}    

    The ReLU stage in the CNN layer is followed by the pooling (POOL) stage. Activations from the ReLU are downsampled by the POOL stage. As shown in Figure 4, pooling layer downsamples the activations spatially and is usually done either by averaging or by computing the max over a small window of activations produced by the ReLU stage. Downsampling further reduces the number of parameters in the model and hence the amount of computation in the network. It also results in small shift invariance, since a small amount of change in the input will not change the output of a CNN layer, irrespective of whether average or max pooling is used. The use of pooling can be viewed as adding a strong prior that the function the layer learns must be invariant to small translations [13]. Because of the downsampling at this stage, there's a progressive reduction in size of the feature map generated by layers of the CNN, as shown in Figure 2. \\ 

    \noindent
    \textbf{\textit{Forward pass}}\\
    As shown in Figure 2, each CNN layer transforms one volume of activations into another volume during the forward pass of the processing and the system makes a prediction at the right most layer on the model. As discussed above, these activations are generated by doing a dot product of the kernel with part of the input as the kernel convolves over the input. In this scheme, the weight values in a kernel do not change as it convolves across the input. This is a critical design decision of CNNs, because this scheme ensures that any useful features identified in one part of the image can be re-used everywhere else without having to be independently learned, as explained by Murphy [27]. Using the kernel in such a fashion introduces a strong prior probability distribution over the parameters of a layer, which warrants that the weights for one hidden unit must be identical to the weights of its neighbor but shifted in space [13].\\

    \noindent
    \textbf{\textit{Model Training using Back-Propagation}} \\
    Weights in the filters are the parameters of a CNN model and model fitting involves learning these weights using a method called back-propagation. Back-propagation is an optimization algorithm where the goal is to find a set of values for model parameters that generalizes the learning in such a way that the model achieves superior classification or regression accuracy on data that it has not seen before. One of the ways to achieve this goal is by minimizing the error on training dataset by using gradient descent (or its variant) method for optimization. In this mechanism many forward and backward passes are made using mini-batches of training data. A mini-batch is set of input data taken together for making several forward passes and using the average of their error in the back-propagation process for weight adjustments. Use of mini-batches in gradient descent accelerates the training and also helps to obtain an unbiased estimate of the gradient by taking the average gradient on a mini-batch. 

    Activations are generated in the forward pass and the error is calculated by measuring the differential between prediction and target using an error function at the last layer of the model. Gradient descent is used to nudge the prediction close to the target by adjusting weights of the model in the backward pass from the output to the input layer using chain rule of derivatives. Values of the parameters of the model are adjusted in small increments, over many iterations, with the opposite sign of the derivative.

    The hope that a model that has low training error will also have small test error is based upon the assumption that that the examples in each dataset are independent from each other, and that the training set and test set are identically distributed, drawn from the same probability distribution, in other words examples in training and test datasets are drawn in identically and independently distributed (IID) fashion.\\

     %local minima problem
    
    \noindent
    \textbf{\textit{Validation Set and Hyper-Parameter Tuning}}\\
    Like most statistical learning models, CNNs also use hyper-parameters to fine-tune the model. The objective of using optimally calibrated hyper-parameters is to minimize the test error of the model. This ensures that the model learns good generalizations and does not just memorize patterns it sees in the training dataset - a problem called \textit{overfitting}. Two commonly used hyper-parameter of the model are, \textit{learning rate} and \textit{weight decay}. Learning rate determines how big or small adjustment to make to model parameter values during back-propagation process. A high learning rate may help decrease the model error fast, but may also result a sub-optimal solution. A low learning rate on the other hand may achieve optimal model fitting, but training the model may take a long time. A validation set is used in this case to find a learning rate that provides a good compromise between training time and model accuracy. A validation set is a dataset that is reserved to test model accuracy as the model is trained using different hyper-parameters. In this case the training learning rate that results in best accuracy on the validation set is chosen as the final learning rate. Depending upon the availability of data, the validation set may be an independent dataset or a subset of the test dataset. The selection of weight decay hyper-parameter is discussed in next section where I explain the issue of overfitting the model and how to address it using regularization.\\

    \noindent
    \textbf{\textit{Overfitting and Regularization}}\\    
    Because of their large numbers of parameters, CNN models have a proclivity to overfit when not designed with suitable regularization parameters or when there is not sufficient data to train the model. Overfitting results in a model that shows high variance when the model is trained on different training datasets (Hastie \textit{et al}.\ [12]). Because of overfitting in a CNN, the training fails to achieve good model generalizations resulting in good training accuracy, but poor performance on test datasets. Bias in a model is another source of prediction error which is usually caused by wrong selection of model for the task. In case of overfitting, variance rather than bias dominates the estimation error. Regularization addresses the problem of overfitting by reducing variance significantly while increasing the bias only a little bit(Goodfellow \textit{et al}.\ [13]). 

    \(L_1\) and \(L_2\) are two commonly used regularization techniques in machine learning in general, including neural networks. Both of these are ways to introduce parameter norm penalty (the regularization term in the error function) to the error function. If \(J(w; X, y) \) represents the original error function, where \(w\) represents parameters of the model, \textit{X} represents training data and \textit{y} represents expected output vector for the training data, then the following represents error function with the regularization term

    \[ \widetilde{J}(w; X, y) = J(w; X, y) + \alpha \Omega(w)   \]

    Here \(\alpha \Omega(w)\) is the regularization term. Regularization term limits the capacity of the model by ensuring that model parameters values do not become very large or very small between iterations during training. \(\alpha\) in the regularization term is the weight hyper-parameter that determines contribution of the norm penalty term \(\Omega\) relative to the error function. \(L_2\) regularization (also called weight decay) drives the model weights closer to the origin by adding a regularization term \( \Omega(w)\) = 1/2 \( \norm{w}^{2}_{2}  \) to the error function. In \(L_1\) regularization \( \Omega(w)\) is calculated as the sum of absolute values of the individual parameters, \( \norm{w}_{1} \). 

    The dropout technique proposed by Srivastava \textit{et al}.\ [14] is another regularization technique used specifically with neural networks. In this technique units are randomly dropped (along with their incoming and outgoing connections) from the network during training to prevent units from co-adapting too much.

    In my implementation of a CNN for the ``localizing little landmarks'' task, I explored \(L_1\) and \(L_2\) regularizations as well as dropout. I report my findings in the experiments section 5.3. Singh \textit{et al}.\ [1] use \(L_2\) regularization as I explain in section 4.3.

     \section{Object Detection in R-CNN (Regions with CNN features)}

    \begin{wrapfigure}{l}{0.6\textwidth}
        \begin{center}
            \includegraphics[width=0.48\textwidth]{Images/R-CNN}
        \end{center}
        \caption{R-CNN process for object localization. \textit{This figure is best viewed in color.}}
    \end{wrapfigure}    

     R-CNN [28], a model proposed by Girshick \textit{et al}.\ was one of the first approach to show that a model based on CNN can be effectively used for object localization in an image (Figure 5). In this approach, Girshick \textit{et al}.\ generate 2000 region proposals (using selective search [30]) for the input image and then use CNN model of Krizhevsky \textit{et al}.\ [26] to generate 4096 dimensional feature maps for these regions. These activations are then used for region classification using a Support Vector Machine (SVM).

     While R-CNN has reported impressive performance for prominent object detection, it has not been very successful in detecting small objects in an image. I discuss some variants of R-CNN technique that other researcher have used for small object localization [6,8] in the next chapter.

    \chapter{Related Work}
    \doublespacing
    The problem of localization of small objects in an image using CNN has not been widely studied. Chen \textit{et al}.\ [6] extended the R-CNN algorithm to detect small objects in an image --- like a computer mouse on a desk, or a faucet in a kitchen. They used a modified Region Proposal Network (RPN) [7] by choosing object proposals many times smaller than used in the original RPN. To use context information, they also cropped a region centered at the region proposal, but bigger than the region proposal. The region proposal and the context proposal are fed to two parallel CNNs and their concatenated activation are used as an input to a third CNN to make predictions. Chen \textit{et al}.\ used intersection over Union (IoU) as a performance metric, whereas in my work I use the Euclidian distance between original and predicted object coordinates normalized by the bounding box of the object, as I explain in section 4.3. 

    Another important work for small object identification was done by Eggert \textit{et al}.\ [8]. These authors modified the Faster R-CNN model [7] to leverage higher-resolution feature maps for brand logo detection. Their work qualifies as small object detection, since they are trying to locate small brand logos in pictures such as images of a soft drink brand in a picture taken at an outdoor concert venue, or images of a sport brand on a person's shirt who is walking a dog. In their work they attempt to generate better region proposals and assume a perfect classifier. They compare performances of region proposal generators using activations from \textit{conv3}, \textit{conv4} and \textit{conv5} layers of a pre-trained VGG-16 model and discover that \textit{conv3} and \textit{conv4} layers' activations performed better than activations of \textit{conv5} layer. 

    The research that I extend [1] in this work proposes an architecture that is recurrent in the sense that the feature map generated by one step of the model is encoded as contextual information and is fed as input to the next step in the sequence along with the feature map generated by the convolutional layer. 

    Another important work that explores this idea of using contextual information is by Zuo \textit{et al}.\ [9]. In their work they argue that convolutional and pooling layers in a CNN are performed locally without considering other regions of the image and therefore fail to capture contextual dependencies for better representation. They propose a model that encodes this correlation for better performance.

    In this thesis I use transfer learning to demonstrate that instead of training a network from scratch, using a pre-trained network may reduce  the training time significantly. Pan \textit{et al}.\ in [10] do an in depth study of feasibility of transfer learning and show that the knowledge learned by a model in one domain can be transferred to another machine learning model in a different domain even when the feature space and/or the data distribution of source and target systems is not the same. Shin \textit{et al}.\ [11] also employ transfer learning to fine-tune a CNN model pre-trained on natural image dataset (RGB) to a medical image (monochrome) classification task. Transfer learning of this kind has been successfully used by numerous researchers and practitioners in image classification and localization tasks by exploiting patterns learned by deep CNN models pre-trained on large image datasets. 

    \chapter{Detecting a Small Object in an Image}
    \doublespacing
    An efficient object detection model may use region proposals, as  one used by Girshick \textit{et al}.\ in R-CNN [28, 7, 15]. Their two stage architecture is current state-of-the-art where the first stage extracts region proposals and the second stage uses these region proposals as input to a CNN which is pre-trained on an auxiliary dataset. This setup has shown remarkable performance improvement for a prominent object detection in an image, but is not suitable for little landmarks, as little landmarks do not have very distinctive features. A simple solution to address the issue of lack of distinctness of little landmark in an image can be either to magnify the image or take a high resolution image. As explained by Eggert \textit{et al}.\ [8], this simple approach may not be very effective since computing convolutions in a CNN grows quadratically with image dimensions and will result in unnecessary computation. Moreover, as explained by Chen \textit{et al}.\ [6], low resolution inputs for small objects are deeply embedded  in the nature of visual perception and a robust vision system should be able to deal with it.
        
    This limitation of R-CNN beckons a different deep learning architecture to localize small objects in an image. Singh \textit{et al}.\ [1] propose a stepwise regression model for this task and they call this model an architecture for localizing little landmarks. They propose a recurrent model that is trained end-to-end supervised by location of the little landmark in an image. Authors demonstrate the robustness of this model by testing and training it on several datasets. One contribution of their work was to annotate a dataset that contains images with little landmarks. Authors use Stanford's car dataset [15] for this task and annotate images in the dataset with the location of the car door handle. We use this Car Door Handle (CDH) dataset and our own Dog Walking Images (DWI) dataset for training and testing our models. Details of these datasets are provided in chapter 5.

    \section{Architecture Overview of Little Landmark CNN Model}
    The CNN architecture for the detection of little landmarks [1] exploits the fact that a little landmark is defined by its context. Authors define a \textit {latent landmark} as a location that are more prominent than the little landmark and can be used as context to detect the little landmark conditionally. The localization in this architecture happens in a sequence of steps where a series of latent landmarks lead to the target little landmark by providing contextual information. Figure 6 illustrates this process with two examples. In the image on the left hand side, the car door handle is detected in a sequence of three steps where a trained network finds the first latent landmark (in red) near the front wheel, the second latent landmark in the bottom half of the front door (in green) which helps find the target location (in blue) in the third step. Similar pattern can be seen in the image on the right hand side of Figure 6 where the edge of the switchboard is used as the starting point to localize the light switch on the wall in three steps.

    \begin{figure}[h!]
    \centering
        \begin{subfigure}[b]{0.45\linewidth}
            \includegraphics[height=4.5cm, width=6.75cm]{Images/Figure4a-CDH}
            \caption{Car Door Handle Localization.}
        \end{subfigure}
        \begin{subfigure}[b]{0.45\linewidth}
            \includegraphics[height=4.5cm, width=6.75cm]{Images/Figure4b-LS}
            \caption{Light Switch Localization.}
        \end{subfigure}
        \caption{Localization of the \textit{little landmark} in three steps. The model discovers \textit{latent landmarks} in first two steps that help find coordinates of the target in the third step. For the car door handle localization, in the first steps, the model localizes near the front wheel of the car, gets little closer to the car door handle in second step and lands on the target in the third step. For the light switch localization, the process starts from the edge of the switchboard and the model detects the light switch in three steps. \textit{This figure is best viewed in color.}}
        \label{fig:localization}
    \end{figure}

    Figure 7 shows a schematic diagram of the little landmark localization architecture that localizes a car door handle in three steps. In this recurrent CNN architecture, the bottom convolutional layers are used to extract a feature map of the input image which becomes the common input to all steps on the top.  The three columns in the upper portion of Figure 7 correspond to the stepwise procedure that detects little landmark in the last step. Each prediction step \textit{s} in the model generates location of the latent landmark (red blob) and predicts the location of next latent landmark (blue blob). The prediction for the next step is encoded as a feature map \(P^{(s)}\) and along with the image feature map (generated at the bottom half of Figure 7) is used as input to the next step. The loss calculation for the model is explained later in section 4.3, but it's worth noting here that the difference between prediction (blue blob) in step \textit {s-1} and the generated latent landmark location (red blob) in step \textit {s} contributes to total error of an iteration.

    \begin{figure}[h]
      \centering
      \includegraphics[width=12cm, height=12cm]{Images/Figure5-Architecture}
      \caption{Little Landmark CNN Architecture. In this architecture, an image enters the system from the bottom and the final prediction for the door handle of the car is made in step 3 of the process. The input image goes thru six layers of CONV-RELU transformations. The output from CONV6/RELU6 layer is an input to all three prediction steps above. The red blob is the prediction for a latent landmark in that step and is used to predict the location of next latent landmark. This information is encoded as a feature map with radial basis kernel (blue blob) and is passed as a feature to the next step. Coordinates of the little landmark are predicted in the last step. CONV/RELU boxes of the same color shows that parameters are shared in those layers. \textit{This figure is best viewed in color.}}
      \label{fig:arch}
    \end{figure}            

    Weights in row two and three of all steps are shared. It must be noted that this CNN model does not use the pooling layer. Although authors do not explain why, but I surmise that since pooling results in a low resolution feature map than the original one, it may make detection of the little landmark more difficult as it already has non-distinctive characteristics.

    \section{Image Pre-processing}
    A batch of ten images is created as the input to the model. Height and width of images are adjusted so that all images in a batch have same dimensions. When necessary, images are zero padded to match the longest width and height among all images in a batch. Images are normalized by performing mean subtraction and standard deviation scaling. Random scale jitter is added to images and images are also randomly flipped to introduce noise for minimizing the problem of overfitting on training data.

    A location grid, \textit{\texttt{out\_locs}},  is created for each batch which corresponds to spatial dimension of the output from the CONV6/RELU6 layer of the bottom portion of the model. This location grid is used for generating the latent landmarks (red blobs in Figure 7) as explained in the next section.

    \section{Prediction Model}
    Coordinates of the target little landmark is the only supervision used in the model and the model learns to infer latent landmarks. As explained before, each step in the model learns to find its latent landmark and predicts the location of the latent landmark for the next step. I explain this process next.\\

    \noindent
    \textbf{\textit{Latent Landmark Centroid Generation (red blob in Figure 7)}}\\
    The CONV11/RELU11 layer of step \textit {s} produces an output of dimension \textit {w} x \textit {h} x \textit {26}, where value of \textit {w} and \textit {h} is determined by the width and height of images in the input batch. The output therefore has 26 \textit {w} x \textit {h} dimensional layers. The product \textit {l} = \(w*h\) corresponds to the location grid, \textit{\texttt{out\_locs}}, created during the pre-processing stage. Network output \(z_l^{(s)}\) from \textit {l} locations of the first layer of \textit {w} x \textit {h} x \textit {26} dimensional output is used to calculate the latent landmark \(pc\) of step \textit {s} as follows    

    \[ pc^{(s)} = \displaystyle\sum_{i=0}^{l} q^{(s)}_i * \textit{out\_locs}_i \tag{1}\]

    \noindent
    where \(q_i^{(s)}\) is softmax over all locations computed as \(q_i^{(s)} = e^{z_i^{(s)}}/\ \Sigma_i e^{z_i^{(s)}} \) and \(z_i^{(s)} \in  \mathbb{R} \) is the output from network for confidence at \(l_i \).\\

    \noindent
    \textbf{\textit{Latent Landmark Prediction for Next Step (blue blob in figure 7)}}\\
    Each step produces an estimate of the next step's latent landmark, \(P^{(s)}\). \(pc^{(s)} \) calculated in (1) along with the prediction \(poc\) made by each location \(l_i\) is used for generating \(P^{(s)}\). \textbf{The sum \(pc + poc\) predicts the centroid for the next step's latent landmark, except for the last step. In the last step, \(pc + poc\) predicts the location of the little landmark}.

    To calculate \(poc\), a logical grid of \textit {5} x \textit {5} cells is placed over each location \(l_i\). The last 25 layers of \textit {w} x \textit {h} x \textit {26} dimensional output from CONV11/RELU11 is used for this calculation. This scheme is illustrated in Figure 8.

    \begin{figure}[t]
      \centering
      \includegraphics[width=\linewidth]{Images/Figure6-Grid}
      \caption{Latent Landmark Prediction Grid. A separate logical grid (\textit{5} x \textit{5} in size shown on the left) is placed over each coordinate produced by CONV6/RELU6 layer (shown on the right). The grid has fixed values for \textit{x} and \textit{y} coordinated of each 25 grid points. The grid values when multiplied by the confidence value generated by the network predicts the \textit{latent landmark} for the next step. \textit{This figure is best viewed in color.}}
      \label{fig:grid}
    \end{figure}

    The left hand side of Figure 8 shows 25 logical grid points \(g_j\) corresponding to a locations \(l_i\) (as shown on right hand side of Figure 8), where \(g_j(x), g_j(y) \in \{-50, -25, 0, 25, 50\} \) pixels. The network produces 25 confidence values \(o_{ji}^{(s)}\) for \(j \in \{1, ..., 25\} \) from the last 25 layers of the output from CONV11/RELU11 layer. These \(o_{ji}^{(s)}\) values are the softmax of network outputs. Each location \(l_i\) produces an estimate for the next latent landmark as follows

    \[      p_i^{(s)} = \displaystyle\sum_{j=0}^{25} o_{ji}^{(s)} * g_j \tag{2} \]

    \noindent
    These \( p_i^{(s)} \) values are used with confidence values \(q_i^{(s)} \), produced by first layer of the output from CONV11/RELU11 layer, to produce \( poc \) as follows

    \[      poc^{(s)} = \displaystyle\sum_{i=0}^{l} q_i^{(s)} * p_i^{(s)} \]

    \noindent
    Finally, to generate prediction \(P^{(s)} \) for the latent landmark of the next step, encoding is done by placing a radial basis kernel with \(\beta \) = 15, centered at \( pc + poc \). In the last step, \( pc  + poc \) predicts the location of the little landmark --- coordinates of the car door handle in Figure 8.\\

    \noindent
    \textbf{\textit{Training (and Loss Calculation)}}\\
    We train the model through back-propagation using Adam optimizer. \(L_2\) loss between the predicted location and the ground truth is used for gradient calculation. However, there are several components in the loss value. \(L_2\) loss between the ground truth and the location predicted by the last step of the model and the \(L_2\) loss between latent landmarks predicted in step \(s\) and estimated in step \textit {s-1} contribute to the total loss of the model in one iteration, which is averaged over the minibatch of ten images to perform back-propagation of error through the network. The weightage given to the step losses is less than the weightage given to \(L_2\) loss between the ground truth and the predicted location in the last step as shown below

    \[ L =  \displaystyle\sum_{s=1}^{S} \lambda _s L^{(s)} + R(\theta)\]

    \noindent
    I used \( \lambda _s = 0.1 \), except for the final step \(S\) where \( \lambda _s = 1\).  \( R(\theta) \) is a regularizer for the parameters of the network. I used \(L_2\) regularization of network weights with a multiplier of \(0.00005\). 
        
    Incorporating losses from all steps of the model in the loss function encourages earlier steps to be informative for the later steps by penalizing disagreement between the predicted and later detected latent landmark locations. Also, encoding predictions as a feature map instead of as a rigid constraint for the next step allows it to ignore the prediction from the previous step if necessary. This flexibility is helpful in early stages of the training when the latent landmark estimates are not very reliable.\\

    \noindent
    \textbf{\textit{Performance Metric}}\\
    To evaluate the model accuracy, Singh \textit{et al}.\ used a generally accepted [17] metric of plotting detection rate against normalized distance. The \(L_2\) norm between actual and predicted locations is normalized by the height of the bounding box of the car in the car door handle dataset,  and  by height of the switchboard in the light switch dataset.\\

    \chapter{Methods}
    \doublespacing
    Authors of the original paper provided their Matlab implementation for the model. I used that codebase to create our own implementation of the model using TensorFlow\textsuperscript{\textregistered} [16] and Python. I tested the fidelity of my implementation by comparing my test results with results that authors published in their paper [1]. I used the same dataset as the authors' used for training and testing the model and I found that my model performed similar to authors'. Figure 9 shows a comparison between the models. \\

    \begin{figure}[h!]
    \centering
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[height=4.5cm, width=6.75cm]{Images/Figure7a-Original}
            \caption{Original}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\linewidth}
            \includegraphics[height=4.5cm, width=6.75cm]{Images/Figure7b-TF}
            \caption{TensorFlow}
        \end{subfigure}
        \caption{Original vs. Tensorflow\textsuperscript{\textregistered} test accuracy comparison. Graph on the left shows plot of normalized distance vs. detection rate published by Singh \textit{et al}.\ The curve marked as Pred 3 shows accuracy of the three step process. Tensorflow\textsuperscript{\textregistered} implementation's test accuracy is plotted on the right side and shows comparable performance to Pred 3 curve.}
        \label{fig:compare}
    \end{figure}

    The plot of normalized distance vs. detection rate from the Tensorflow\textsuperscript{\textregistered} [16] implementation is depicted in Figure 7(b). Pred 3 plot in Figure 7(a) shows authors' model accuracy. Pred 2 and Pred 1 plots in Figure 7(a) are of model with two steps and one step respectively. My implementation shows comparable accuracy to the original model.\\

    \noindent
    \textbf{\textit{Datasets}}

    \noindent
    \textit{Car Door Handle (CDH) Dataset}\\
    I received an annotated dataset of CDH dataset from authors. There were 1920 training images and 1200 testing images in the dataset. All images had the coordinates of the car door handle annotated. Car bounding box details were also available.\\

    \noindent
    \textit{Dog Walking Images (DWI) Dataset}\\
    I used our own dog walking images to test model's effectiveness on a different dataset. I used 310 images for training and 70 for testing. PSU researchers annotated coordinates of hand-holding-leash little landmark of the image and also the leash bounding box in the image.\\

    \noindent
    \textbf{\textit{Training Detail}}\\
    I tested the Tensorflow\textsuperscript{\textregistered} model on a Linux computer with a GPU. I trained the model for 3100  epochs where each epoch was trained on a shuffled set of 1920 images in batches of ten images each. It took about 107 hours to train the model.

    \section{Model Modifications}

    \noindent
    \textbf{\textit{Transfer Learning}}\\
    Fergus \textit {et al}.\ [4] and many other researchers [18, 19] have demonstrated that in a well-trained CNN model lower layers get attuned to more generic features of an image like edges, corners or color blobs whereas higher layers learn more specific features like a person's face or wheels of a car. Bengio [18] argues that a deep learning algorithm seeks to discover good representations at multiple layers  in such a way that features learned in a higher level can be composed of features learned in lower layers of the network. Another important characteristic of learning in this scheme is that features learned in higher layers are invariant to variations in training distribution --- like background, viewpoint, scene context etc. Fergus \textit{et al}.\ [4] by mapping activations in a CNN model back to its pixel space experimentally demonstrate this fact as shown in Figure 10. Properties of a deep CNN discussed above make transfer learning possible where representations learned with a CNN trained on a large image dataset can be effectively used to  initialize another network. Oquab et al. [19] show that despite differences in image statistics and tasks in two different datasets, transferred representations lead to significant improvement in image detection and classification and they demonstrate that incorporation of prior knowledge via transfer learning can boost the performance of CNNs by a large margin. Because of these advantages two most common reasons to employ transfer learning is that it can reduce training time and that it can overcome overfitting problem when there is paucity of training data.

    \begin{figure}[t]
    \centering
        \begin{subfigure}[b]{0.23\linewidth}
            \centering
            \includegraphics[height=3.8cm, width=3.8cm]{Images/Figure8b-Deconv}
            \caption{Layer 2 activation mapped to its pixel space}
        \end{subfigure}    
        \begin{subfigure}[b]{0.23\linewidth}
            \centering
            \includegraphics[height=3.8cm, width=3.8cm]{Images/Figure8a-Deconv}
            \caption{Image patch corresponding to (a)}
        \end{subfigure}
        \begin{subfigure}[b]{0.23\linewidth}
            \centering
            \includegraphics[height=3.8cm, width=3.8cm]{Images/Figure8d-Deconv}
            \caption{Layer 5 activation mapped to its pixel space}
        \end{subfigure}                
        \begin{subfigure}[b]{0.23\linewidth}
            \centering
            \includegraphics[height=3.8cm, width=3.8cm]{Images/Figure8c-Deconv}
            \caption{Image patch corresponding to (c)}
        \end{subfigure}
        \caption{Visualization of features learned by a CNN model using the Deconvolutional Network. \textit{This figure is best viewed in color.}}
        \label{fig:devonv}
    \end{figure}

    My motivation to use transfer learning was to exploit generalizations learned by VGG-16 network [2] on ImageNet [3] dataset to reduce the training time of for little landmark localization. As I discussed before, the model proposed by Singh \textit{et al}.\ [1] takes more than 100 hours to train. I used transfer learning to improve the training time for this model by using activations from another state-of the-art CNN model, pre-trained on an auxiliary dataset, as the starting point for training the model. I observed significant improvement in training time in this modified implementation that used transfer learning. I evaluated the modified model's accuracy on the car door handle dataset and saw results comparable to the original model.

    At Portland State University we have our own dataset of dog walking images that we use for our computer vision projects. It's a small dataset and its statistics are explained at the beginning of this chapter. Figure 1 shows a typical image from the dog walking dataset. Understanding situations in an image is an ongoing project at Portland State University. Visual situations are concepts such as ``a crowd waiting for a bus,'' or ``a game of ping-pong,'' or ``walking the dog,'' [20]. Another goal of my thesis was to localize ``hand-holding-leash'' part of a dog-walking image. The rational was that detection of ``hand-holding-leash'' situation in the image will give more evidence that a person may be walking a dog in the image. This detection will help localize other objects or relationships in the image, which may help to identify visual situation in an image. To this end, I used the design proposed to detect little landmarks [1] to localize ``hand-holding-leash'' part of the dog walking image. 

    I used transfer learning to overcome the problem of overfitting because of scarce training data in our dog walking images dataset --- we did encounter overfitting problem when we trained the model from scratch with limited data in dog walking images dataset. \\

    \noindent
    \textbf{\textit{Little Landmarks Model Modification using Transfer Learning}}\\                   
    As explained before, I used generalizations learned by the VGG-16 model for training the car door handle as well as the dog walking image datasets. VGG-16 [2] is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford that achieved 92.7\% accuracy in top-5 test category and 70.5\% accuracy in top-1 category on the Imagenet dataset. The Imagenet dataset [3] is a dataset of over 14 million images belonging to 1000 categories.   

    \begin{figure}[h]
      \centering
      \includegraphics[width=\linewidth]{Images/Figure10-VGG-16}
      \caption{VGG-16 architecture. The modified model used activations from \textit{pool4} layer of the architecture depicted in column D on the right. \textit{This figure is best viewed in color.}}
      \label{fig:vgg}
    \end{figure}    

    Figure 11 shows a schematic diagram of the VGG-16 model. the VGG-16 CNN architecture has sixteen weight layers (model D on the right hand side in Figure 11) and a filter of size \textit{3} x \textit{3} is used to convolve in all layers. The input to the network is a fixed size \textit{224} x \textit{224} RGB image. Mean image subtraction is the only preprocessing performed on input images to center images. I used activations from \textit{pool4} layer of the model (the layer with dimensions \textit{14} x \textit{14} x \textit{512} on the left hand side in Figure 10) as input to the modified little landmarks model (Figure 12).     

    \begin{figure}[h]
      \centering
      \includegraphics[width=12cm, height=9cm]{Images/Figure11-Modified-LL}
      \caption{Little landmarks architecture modified using transfer learning. This model used features learned by the VGG-16 network as an input to rest of the model. As before, an image enters the system from the bottom and the final prediction for the door handle of the car is made in step 3 of the process. The bridge layer is use to transform output dimensions of VGG-16 model so that it dovetails with the upper part of Little Landmarks' architecture. \textit{This figure is best viewed in color.}}
      \label{fig:tl_arch}
    \end{figure}        

    In the modified architecture I used a bridge layer that takes \textit{pool4} activations from the VGG-16 model as input and performs a layer of convolutions to modify output dimensions so that it conforms with rest of the architecture. I used a publicly available pre-trained VGG-16 model from Github [21] as the basis of my model. I used same image preprocessing that was used for the original little landmark model and fed a mini-batch of ten images to VGG-16 model, as before. Internally the VGG-16 implementation warped images to 224 x 224 x 3 before feeding it to the network.    

    \section{Experiments and Results}

    \noindent
    \textbf{Experiment 1}\\    
    In this experiment I used the modified model that uses VGG-16 model with transfer learning (Figure 12). I trained the model on the same car door handle dataset as the original model and 

    \begin{wrapfigure}{l}[9mm]{0.5\textwidth}
        \begin{subfigure}[b]{\linewidth}
            \includegraphics[height=5cm, width=7.5cm]{Images/Figure12a-Exp1}
            \caption{Experiment 1}
        \end{subfigure}
        \begin{subfigure}[b]{\linewidth}
            \includegraphics[height=5cm, width=7.5cm]{Images/Figure12b-Exp1}
            \caption{Experiment 2}
        \end{subfigure}
        \begin{subfigure}[b]{\linewidth}
            \includegraphics[height=5cm, width=7.5cm]{Images/Figure12c-Exp1}
            \caption{Experiment 3}
        \end{subfigure}        
        \caption{Experiment Results.}
    \end{wrapfigure}   

    \noindent
    found that performance of both models were comparable. In this training weights of the pre-trained VGG-16 model were fixed and fine tuning was performed only for the weights in regression steps of the model. The modified model needed about 46 hours to train for 530 epochs. I observed about 57\% improvement in training time of the model. Figure 13(a) shows the normalized distance vs. detection rate plot for this experiment.

    \noindent
    \textbf{Experiment 2}\\         
    In this experiment I trained the same network as in experiment 1 with dog walking images. 310 training and 70 test images were used in this experiment to localize hand-holding-leash part of the image. I trained the model for 96 hours and saw near perfect training accuracy. However, I saw very poor test accuracy as shown in Figure 13 (b).

    The model appeared to overfit by memorizing features of training images and it did not seem to learn any general pattern for localization. I trained the model with weight loss values of 0.005 and 0.0005 also in addition to 0.00005, which was used for \(L_2\) regularization in the original model. I also experimented with \(L_1\) regularization. None of these method resulted in any performance improvement. I also used dropouts [14] for regularization, but did not see any performance improvement.

    Since context plays an important role in little landmark's localization I changed the model to localize in five steps instead of three. My rational for this approach was that a model localized in five steps may pick up better context information which may result in better generalization. However, this approach also did not work.

    Another unsuccessful attempt to improve performance was to use activations from \textit{pool3} layer of the VGG-16 model. The rational for this approach was that since latent and little landmarks are both not very distinctive, using filters from a lower layer, that may be more attuned to features of latent and little landmarks, may show better performance. \\

    \begin{wraptable}{r}{0.55\textwidth}
    \centering
    \tiny
    \begin{tabular}{ | p{1.4cm} | p{1.4cm} | p{1.4cm} | p{1.4cm} |  p{1.4cm}| }
    \hline
    %Normalized Distance &  &  Detection Rate & & \\ \hline
    Normalized Distance &  \multicolumn{4}{|c|}{Detection Rate} \\ \hline
    & Original & Experiment1 & Experiment2 & Experiment3 \\ \hline
    0.01 &  0.29 & 0.28 & 0.00 & 0.01 \\ \hline
    0.02 & 0.71 & 0.49 & 0.00 & 0.02 \\ \hline
    0.03 & 0.81 & 0.73 & 0.00 & 0.12 \\ \hline
    0.04 & 0.86 & 0.83 & 0.00 & 0.23 \\ \hline
    0.05 & 0.90 & 0.91 & 0.00 & 0.44 \\ \hline
    0.06 & 0.88 & 0.94 & 0.14 & 0.46 \\ \hline
    0.07 & 0.90 & 0.94 & 0.14 & 0.65 \\ \hline
    0.08 & 0.93 & 0.95 & 0.14 & 0.66 \\ \hline
    0.09 & 0.93 & 0.97 & 0.43 & 0.75 \\ \hline
    0.10 & 0.97 & 0.98 & 0.43 & 0.75 \\ \hline

    \end{tabular}
    \caption{Experiment Results}
    \label{table:exp}
    \end{wraptable}    

    \noindent
    \textbf{Experiment 3}\\             
    In this experiment I trained the original TensorFlow model that I discussed in section 4.3, but with same amount of training data that was used for experiment 2. I anticipated that training the original little landmark model with small amount of data may render the model not as effective. For about the same duration of training, I did notice inferior performance in this experiment, but the performance was not as poor as in experiment 2. Figure 13(c) shows normalized distance vs. detection rate plot for this experiment. 

    Table 1 shows a comparison of all model performances.\\

    \chapter{Conclusion}
    \doublespacing
    As I discussed before little landmarks in an image do not have very distinctive features and the use of context is an effective technique for their detection. I demonstrated this by implementing the architecture originally proposed by Singh \textit{et al}.\ [1] and also by modifying their design to incorporate transfer learning. By using transfer learning I was able to show significant improvement in training time while not degrading performance of the model. I also tried to exploit transfer learning to overcome the problem of overfitting because of insufficient training data in the dog walking images dataset. Although many researchers [19, 22, 23] have been successful in this endeavor, I did not see that upturn in experiment 2. I next explain why this approach did not work in my case.\\

    \begin{figure}[h]
      \centering
      \includegraphics[width=\linewidth]{Images/Figure13-CDH}
      \caption{Representative Images from Car Door Handle Dataset. \textit{This figure is best viewed in color.}}
      \label{fig:cdh}
    \end{figure}        

    \begin{figure}[h]
      \centering
      \includegraphics[width=\linewidth]{Images/Figure14-DWI}
      \caption{Representative Images from Dog Walking Images Dataset. \textit{This figure is best viewed in color.}}
      \label{fig:dwi}
    \end{figure}            

    The comparison of results between experiment 1 and experiment 3 (Table 1) demonstrates that lack of data does impact a model accuracy. However, comparison between results of experiment 2 and experiment 3 shows that lack of training data resulted in much worse performance in dog walking images as compared to cars images. Figure 14 shows some representative images of cars used in the training to localize the car door handle. In all these images location of the door handle has such a regular spatial configuration that latent landmarks can be consistently found, in other words the context for little and latent landmarks are consistent across images. We discovered that for a three-step localization, the first latent landmark was found near the front or rear wheel, the second in the door, which helped in the target detection in the third step. Whereas in dog walking images (Figure 15), the context around the ``hand-holding-leash'' landmark is not consistent between images and as a result the model does not train as well as the car door handle dataset model does.

    \chapter{Future Work}
    \doublespacing
    A hallmark of CNN models is their ability to learn invariant features of an image so that the prediction or classification accuracy is not impacted by variability in an image. Xu \textit {et al}.\ [24] explain that representations in very top layer of a CNN distills information in an image down to the most salient objects. However this property of CNN is not very useful for tasks like small object detection since they are defined more by the context. They propose an attention based model [24] where they argue that using low level representations may help preserve the semantic information in an image. In their work they draw a parallel between this approach and presence of attention in the human vision system. They use this insight to create a Long Short Term Memory (LSTM) based CNN system that automatically learns to describe the content of images. I argue that a similar approach of using attention based model can be explored for identification for small objects in an image.

    The variant of transfer learning that I used is supervised transfer learning where the source network was trained with supervision. Another form of transfer learning uses unsupervised pre-training. Erhan \textit {et al}.\ [25] explain that greedy layer-wise unsupervised pre-training overcomes the challenges of deep learning by introducing a useful prior to the supervised fine-tuning training procedure, which effectively puts parameter values in an appropriate range for further supervised training. Use of this approach makes unsupervised learning a special kind of regularizer that minimizes the variance at the cost of introducing some bias in the model. This is another promising method for small object localization specially in cases where training data is scarce and context around the small object is not consistent.

    Another slightly unrelated future work is development of a better metric to measure effectiveness of small object detection. Chen \textit {et al}.\ [6] discuss that one of the challenges of small object identification is that we do not know how difficult this task is or how well do existing object detectors work. We found that metric used for measuring performance of small object detection is not consistent --- some researchers use Intersection over Union (IoU) whereas others use some sort of normalization for Euclidian distance between the prediction and the ground truth. A good metric therefore can be very useful tool in this area of work.

    \begin{thebibliography}{9}
        \bibitem{Singh2016}
          Saurabh Singh, Derek Hoiem and David Forsyth,
          \textit{Learning to Localize Little Landmarks},
          Computer Vision and Pattern Recognition,
          2016.          

        \bibitem{Zisserman2015}
          Andrew Zisserman, Karen Simonyan,
          \textit{Very Deep Convolutional Networks for Large-Scale Image Recognition},
          Computer Vision and Pattern Recognition,
          2015.                    

        \bibitem{Russakovsky2015}
          Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei,
          \textit{ImageNet Large Scale Visual Recognition Challenge},
          Computer Vision and Pattern Recognition,
          2015.                    

        \bibitem{Fergus2013}
          Rob Fergus, Matthew D Zeiler,
          \textit{Visualizing and Understanding Convolutional Networks},
          Computer Vision and Pattern Recognition,
          2013.                              

        \bibitem{Fergus2011}
          Rob Fergus, Matthew D Zeiler,
          \textit{Adaptive deconvolutional networks for mid and high level feature learning},
          International Conference on Computer Vision,
          2011.

        \bibitem{Chen2017}
          Chen C., Liu MY., Tuzel O., Xiao J.,
          \textit{R-CNN for Small Object Detection},
          Asian Conference on Computer Vision,
          2017.          

        \bibitem{Ren2016}
          Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun,
          \textit{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
          Computer Vision and Pattern Recognition,
          2016.                    

        \bibitem{Eggert2017}
          Christian Eggert, Dan Zecha, Stephan Brehm, Rainer Lienhart,
          \textit{Improving Small Object Proposals for Company Logo Detection},
          Computer Vision and Pattern Recognition,
          2017.                              

        \bibitem{Zuo2015}
          Zhen Zuo, Bing Shuai, Gang Wang, Xiao Liu, Xingxing Wang, Bing Wang,Yushi Chen,
          \textit{Convolutional recurrent neural networks: Learning spatial dependencies for image representation},
          Conference on Computer Vision and Pattern Recognition Workshops,
          2015.                                        

        \bibitem{Pan2009}
          Sinno Jialin Pan and Qiang Yang.
          \textit{A Survey on Transfer Learning},
          IEEE Transactions on Knowledge and Data Engineering,
          2009.                                                  

        \bibitem{Shin2016}
          Hoo-Chang Shin, Holger R. Roth, Mingchen Gao, Le Lu, Ziyue Xu, Isabella Nogues, Jianhua Yao, Daniel Mollura, Ronald M. Summers,
          \textit{Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning},
          Computer Vision and Pattern Recognition,
          2016.                              

        \bibitem{Hastie2014}
          Trevor Hastie, Gareth James, Daniela Witten, Robert Tibshirani,
          \textit{An Introduction to Statistical Learning},
          Springer,
          2014.                                        

        \bibitem{Goodfellow2016}
          Ian Goodfellow, Yoshua Bengio, Aaron Courville,
          \textit{Deep Learning},
          MIT Press,
          2016.                                        
           
        \bibitem{Srivastava2014}
          Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov,
          \textit{Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
          Journal of Machine Learning Research,
          2014.                               

        \bibitem{SKrause2013}
           Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei,
          \textit{Object Representations for Fine-Grained Categorization},
          4th International IEEE Workshop on  3D Representation and Recognition,
          2013.                                         

        \bibitem{tensorflow2016}
           Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
          \textit{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
          Distributed, Parallel, and Cluster Computing,
          2016.                                                   

        \bibitem{Godil2014}
          Afzal A. Godil, Roger V. Bostelman, William P. Shackleford, Tsai Hong Hong, Michael O. Shneier 
          \textit{Performance Metrics for Evaluating Object and Human Detection and Tracking Systems },
          NIST Interagency/Internal Report,
          2014.                 

        \bibitem{Bengio2017}
          Yoshua Bengio,
          \textit{Deep Learning of Representations for Unsupervised and Transfer Learning},
          Proceedings of Machine Learning Research,
          2017.                               

        \bibitem{Oquab2014}
          Maxime Oquab, Leon Bottou, Ivan Laptev1, Josef Sivic1,
          \textit{Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks},
          Computer Vision and Pattern Recognition,
          2014.

        \bibitem{Quinn2017}
          Max H. Quinn, Erik Conser, Jordan M. Witte, Melanie Mitchell,
          \textit{Semantic Image Retrieval via Active Grounding of Visual Situations},
          Computer Vision and Pattern Recognition,
          2017.          

        \bibitem{machrisaa2017}
          machrisaa,
          \textit{VGG-16 TensorFlow},
          https://github.com/machrisaa/tensorflow-vgg,
          2017.                    

        \bibitem{Ahmed2008}
          Amr Ahmed, Kai Yu, Wei Xu, Yihong Gong, Eric Xing,
          \textit{Training Hierarchical Feed-Forward Visual Recognition Models Using Transfer Learning from Pseudo-Tasks},
          Springer,
          2008.                                        
\          
        \bibitem{Collobert2011}
          Ronan Collobert,Jason Weston,
          \textit{Natural Language Processing (Almost) from Scratch},
          Journal of Machine Learning Research,
          2011.                             

        \bibitem{Xu2016}
          Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio,
          \textit{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
          Computer Vision and Pattern Recognition,
          2016.                                

        \bibitem{Erhan2010}
          Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, Samy Bengio,
          \textit{Why does Unsupervised Pre-training Help Deep Learning?},
          Journal of Machine Learning Research,
          2010.                                       

        \bibitem{Krizhevsky2012}
          Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton,
          \textit{ImageNet Classification with Deep Convolutional Neural Networks},
          Neural Information Processing Systems,
          2012.        

        \bibitem{Murphy2012}
          Kevin Murphy,
          \textit{Machine Learning},
          MIT Press,
          2012.            

        \bibitem{Girshick2014}
          Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik,
          \textit{Rich feature hierarchies for accurate object detection and semantic segmentation},
          Computer Vision and Pattern Recognition,
          2014.                                           

        \bibitem{Hastie2014}
          Trevor Hastie, Robert Tibshirani, Jerome Friedman,
          \textit{The Elements of Statistical Learning},
          Springer,
          2008.                         

        \bibitem{Uijlings2013}
          J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders,
          \textit{Selective search for object recognition},
          IJCV,
          2013.                         

    \end{thebibliography}


\end{document}